{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56e7e1e9-700a-4fcb-b208-8392b530f81d",
   "metadata": {
    "id": "56e7e1e9-700a-4fcb-b208-8392b530f81d"
   },
   "source": [
    "# Fruit Segmentations with Foundations\n",
    "<br><img width=\"1024\" src=\"https://lacoro.org/assets/img/2025/banner_lightmode.png?t=1765113906926\">\n",
    "\n",
    "\n",
    "This activity looks to give an introductory experience to precision agriculture using computer vision techniques.\n",
    "\n",
    "\n",
    "\n",
    "Objectives:\n",
    "- Understand foundation models.\n",
    "- Understand limitations of foundations models in specific applications.\n",
    "- Learn how to apply SAM for segmentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ae39ff",
   "metadata": {
    "id": "a1ae39ff"
   },
   "source": [
    "# Object masks from prompts with SAM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a4b25c",
   "metadata": {
    "id": "b4a4b25c"
   },
   "source": [
    "The Segment Anything Model (SAM) predicts object masks given prompts that indicate the desired object. The model first converts the image into an image embedding that allows high quality masks to be efficiently produced from a prompt.\n",
    "\n",
    "The `SamPredictor` class provides an easy interface to the model for prompting the model. It allows the user to first set an image using the `set_image` method, which calculates the necessary image embeddings. Then, prompts can be provided via the `predict` method to efficiently predict masks from those prompts. The model can take as input both point and box prompts, as well as masks from the previous iteration of prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ab8c70",
   "metadata": {
    "id": "18ab8c70"
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\n",
    "\"\"\"\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/facebookresearch/segment-anything/blob/main/notebooks/predictor_example.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "\"\"\"\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "644532a8",
   "metadata": {
    "id": "644532a8"
   },
   "source": [
    "## Environment Set-up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61dc9d2e-470b-4312-b969-1616ab912578",
   "metadata": {
    "id": "61dc9d2e-470b-4312-b969-1616ab912578"
   },
   "source": [
    "## In case of using colab\n",
    "Remember to connect to your drive and if you want to repeat several times  this tutorial.\n",
    "Also set `using_colab=True` below and run the cell and be sure to select 'GPU' under 'Edit'->'Notebook Settings'->'Hardware accelerator'.\n",
    "If running locally using jupyter, first install `segment_anything` in your environment using the [installation instructions](https://github.com/facebookresearch/segment-anything#installation) in the repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062db428-653f-4f86-89e5-83559cbc6489",
   "metadata": {
    "id": "062db428-653f-4f86-89e5-83559cbc6489"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "%cd /content/drive/MyDrive\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91dd9a89",
   "metadata": {
    "id": "91dd9a89"
   },
   "outputs": [],
   "source": [
    "using_colab = True\n",
    "if using_colab:\n",
    "    import torch\n",
    "    import torchvision\n",
    "    print(\"PyTorch version:\", torch.__version__)\n",
    "    print(\"Torchvision version:\", torchvision.__version__)\n",
    "    print(\"CUDA is available:\", torch.cuda.is_available())\n",
    "    import sys\n",
    "    !{sys.executable} -m pip install opencv-python matplotlib\n",
    "    !{sys.executable} -m pip install 'git+https://github.com/facebookresearch/segment-anything.git'\n",
    "\n",
    "    !mkdir images\n",
    "    !wget -P images https://raw.githubusercontent.com/facebookresearch/segment-anything/main/notebooks/images/truck.jpg\n",
    "    !wget -P images https://raw.githubusercontent.com/facebookresearch/segment-anything/main/notebooks/images/groceries.jpg\n",
    "    !wget -P images https://raw.githubusercontent.com/facebookresearch/segment-anything/main/notebooks/images/dog.jpg\n",
    "\n",
    "    !wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be845da",
   "metadata": {
    "id": "0be845da"
   },
   "source": [
    "## Set-up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33681dd1",
   "metadata": {
    "id": "33681dd1"
   },
   "source": [
    "Necessary imports and helper functions for displaying points, boxes, and masks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b28288",
   "metadata": {
    "id": "69b28288"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29bc90d5",
   "metadata": {
    "id": "29bc90d5"
   },
   "outputs": [],
   "source": [
    "def show_mask(mask, ax, random_color=False):\n",
    "    if random_color:\n",
    "        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n",
    "    else:\n",
    "        color = np.array([30/255, 144/255, 255/255, 0.6])\n",
    "    h, w = mask.shape[-2:]\n",
    "    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
    "    ax.imshow(mask_image)\n",
    "\n",
    "def show_points(coords, labels, ax, marker_size=375):\n",
    "    pos_points = coords[labels==1]\n",
    "    neg_points = coords[labels==0]\n",
    "    ax.scatter(pos_points[:, 0], pos_points[:, 1], color='green', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)\n",
    "    ax.scatter(neg_points[:, 0], neg_points[:, 1], color='red', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)\n",
    "\n",
    "def show_box(box, ax):\n",
    "    x0, y0 = box[0], box[1]\n",
    "    w, h = box[2] - box[0], box[3] - box[1]\n",
    "    ax.add_patch(plt.Rectangle((x0, y0), w, h, edgecolor='green', facecolor=(0,0,0,0), lw=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23842fb2",
   "metadata": {
    "id": "23842fb2"
   },
   "source": [
    "## Example image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2e4f6b",
   "metadata": {
    "id": "3c2e4f6b"
   },
   "outputs": [],
   "source": [
    "image = cv2.imread('images/truck.jpg')\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30125fd",
   "metadata": {
    "id": "e30125fd"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(image)\n",
    "plt.axis('on')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b228b8",
   "metadata": {
    "id": "98b228b8"
   },
   "source": [
    "## Selecting objects with SAM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb1927b",
   "metadata": {
    "id": "0bb1927b"
   },
   "source": [
    "First, load the SAM model and predictor. Change the path below to point to the SAM checkpoint. Running on CUDA and using the default model are recommended for best results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e28150b",
   "metadata": {
    "id": "7e28150b"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from segment_anything import sam_model_registry, SamPredictor\n",
    "\n",
    "sam_checkpoint = \"sam_vit_h_4b8939.pth\"\n",
    "model_type = \"vit_h\"\n",
    "\n",
    "device = \"cuda\"\n",
    "\n",
    "sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\n",
    "sam.to(device=device)\n",
    "\n",
    "predictor = SamPredictor(sam)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c925e829",
   "metadata": {
    "id": "c925e829"
   },
   "source": [
    "Process the image to produce an image embedding by calling `SamPredictor.set_image`. `SamPredictor` remembers this embedding and will use it for subsequent mask prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95d48dd",
   "metadata": {
    "id": "d95d48dd"
   },
   "outputs": [],
   "source": [
    "predictor.set_image(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8fc7a46",
   "metadata": {
    "id": "d8fc7a46"
   },
   "source": [
    "To select the truck, choose a point on it. Points are input to the model in (x,y) format and come with labels 1 (foreground point) or 0 (background point). Multiple points can be input; here we use only one. The chosen point will be shown as a star on the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c69570c",
   "metadata": {
    "id": "5c69570c"
   },
   "outputs": [],
   "source": [
    "input_point = np.array([[500, 375]])\n",
    "input_label = np.array([1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91ba973",
   "metadata": {
    "id": "a91ba973"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(image)\n",
    "show_points(input_point, input_label, plt.gca())\n",
    "plt.axis('on')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c765e952",
   "metadata": {
    "id": "c765e952"
   },
   "source": [
    "Predict with `SamPredictor.predict`. The model returns masks, quality predictions for those masks, and low resolution mask logits that can be passed to the next iteration of prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5373fd68",
   "metadata": {
    "id": "5373fd68"
   },
   "outputs": [],
   "source": [
    "masks, scores, logits = predictor.predict(\n",
    "    point_coords=input_point,\n",
    "    point_labels=input_label,\n",
    "    multimask_output=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f0e938",
   "metadata": {
    "id": "c7f0e938"
   },
   "source": [
    "With `multimask_output=True` (the default setting), SAM outputs 3 masks, where `scores` gives the model's own estimation of the quality of these masks. This setting is intended for ambiguous input prompts, and helps the model disambiguate different objects consistent with the prompt. When `False`, it will return a single mask. For ambiguous prompts such as a single point, it is recommended to use `multimask_output=True` even if only a single mask is desired; the best single mask can be chosen by picking the one with the highest score returned in `scores`. This will often result in a better mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47821187",
   "metadata": {
    "id": "47821187"
   },
   "outputs": [],
   "source": [
    "masks.shape  # (number_of_masks) x H x W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c227a6",
   "metadata": {
    "id": "e9c227a6"
   },
   "outputs": [],
   "source": [
    "for i, (mask, score) in enumerate(zip(masks, scores)):\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.imshow(image)\n",
    "    show_mask(mask, plt.gca())\n",
    "    show_points(input_point, input_label, plt.gca())\n",
    "    plt.title(f\"Mask {i+1}, Score: {score:.3f}\", fontsize=18)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa31f7c",
   "metadata": {
    "id": "3fa31f7c"
   },
   "source": [
    "## Specifying a specific object with additional points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d6d29a",
   "metadata": {
    "id": "88d6d29a"
   },
   "source": [
    "The single input point is ambiguous, and the model has returned multiple objects consistent with it. To obtain a single object, multiple points can be provided. If available, a mask from a previous iteration can also be supplied to the model to aid in prediction. When specifying a single object with multiple prompts, a single mask can be requested by setting `multimask_output=False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6923b94",
   "metadata": {
    "id": "f6923b94"
   },
   "outputs": [],
   "source": [
    "input_point = np.array([[500, 375], [1125, 625]])\n",
    "input_label = np.array([1, 1])\n",
    "\n",
    "mask_input = logits[np.argmax(scores), :, :]  # Choose the model's best mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98f96a1",
   "metadata": {
    "id": "d98f96a1"
   },
   "outputs": [],
   "source": [
    "masks, _, _ = predictor.predict(\n",
    "    point_coords=input_point,\n",
    "    point_labels=input_label,\n",
    "    mask_input=mask_input[None, :, :],\n",
    "    multimask_output=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce8b82f",
   "metadata": {
    "id": "0ce8b82f"
   },
   "outputs": [],
   "source": [
    "masks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06d5c8d",
   "metadata": {
    "id": "e06d5c8d"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(image)\n",
    "show_mask(masks, plt.gca())\n",
    "show_points(input_point, input_label, plt.gca())\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93e2087",
   "metadata": {
    "id": "c93e2087"
   },
   "source": [
    "To exclude the car and specify just the window, a background point (with label 0, here shown in red) can be supplied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a196f68",
   "metadata": {
    "id": "9a196f68"
   },
   "outputs": [],
   "source": [
    "input_point = np.array([[500, 375], [1125, 625]])\n",
    "input_label = np.array([1, 0])\n",
    "\n",
    "mask_input = logits[np.argmax(scores), :, :]  # Choose the model's best mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a52282",
   "metadata": {
    "id": "81a52282"
   },
   "outputs": [],
   "source": [
    "masks, _, _ = predictor.predict(\n",
    "    point_coords=input_point,\n",
    "    point_labels=input_label,\n",
    "    mask_input=mask_input[None, :, :],\n",
    "    multimask_output=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfca709f",
   "metadata": {
    "id": "bfca709f"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(image)\n",
    "show_mask(masks, plt.gca())\n",
    "show_points(input_point, input_label, plt.gca())\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e2d5a9",
   "metadata": {
    "id": "41e2d5a9"
   },
   "source": [
    "## Specifying a specific object with a box"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61ca7ac",
   "metadata": {
    "id": "d61ca7ac"
   },
   "source": [
    "The model can also take a box as input, provided in xyxy format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea92a7b",
   "metadata": {
    "id": "8ea92a7b"
   },
   "outputs": [],
   "source": [
    "input_box = np.array([425, 600, 700, 875])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35a8814",
   "metadata": {
    "id": "b35a8814"
   },
   "outputs": [],
   "source": [
    "masks, _, _ = predictor.predict(\n",
    "    point_coords=None,\n",
    "    point_labels=None,\n",
    "    box=input_box[None, :],\n",
    "    multimask_output=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984b79c1",
   "metadata": {
    "id": "984b79c1"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(image)\n",
    "show_mask(masks[0], plt.gca())\n",
    "show_box(input_box, plt.gca())\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ed9f0a",
   "metadata": {
    "id": "c1ed9f0a"
   },
   "source": [
    "## Combining points and boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8455d1c5",
   "metadata": {
    "id": "8455d1c5"
   },
   "source": [
    "Points and boxes may be combined, just by including both types of prompts to the predictor. Here this can be used to select just the trucks's tire, instead of the entire wheel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e2e547",
   "metadata": {
    "id": "90e2e547"
   },
   "outputs": [],
   "source": [
    "input_box = np.array([425, 600, 700, 875])\n",
    "input_point = np.array([[575, 750]])\n",
    "input_label = np.array([0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6956d8c4",
   "metadata": {
    "id": "6956d8c4"
   },
   "outputs": [],
   "source": [
    "masks, _, _ = predictor.predict(\n",
    "    point_coords=input_point,\n",
    "    point_labels=input_label,\n",
    "    box=input_box,\n",
    "    multimask_output=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e13088a",
   "metadata": {
    "id": "8e13088a"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(image)\n",
    "show_mask(masks[0], plt.gca())\n",
    "show_box(input_box, plt.gca())\n",
    "show_points(input_point, input_label, plt.gca())\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ddbca3",
   "metadata": {
    "id": "45ddbca3"
   },
   "source": [
    "## Batched prompt inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6f18a0",
   "metadata": {
    "id": "df6f18a0"
   },
   "source": [
    "SamPredictor can take multiple input prompts for the same image, using `predict_torch` method. This method assumes input points are already torch tensors and have already been transformed to the input frame. For example, imagine we have several box outputs from an object detector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a06681b",
   "metadata": {
    "id": "0a06681b"
   },
   "outputs": [],
   "source": [
    "input_boxes = torch.tensor([\n",
    "    [75, 275, 1725, 850],\n",
    "    [425, 600, 700, 875],\n",
    "    [1375, 550, 1650, 800],\n",
    "    [1240, 675, 1400, 750],\n",
    "], device=predictor.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf957d16",
   "metadata": {
    "id": "bf957d16"
   },
   "source": [
    "Transform the boxes to the input frame, then predict masks. `SamPredictor` stores the necessary transform as the `transform` field for easy access, though it can also be instantiated directly for use in e.g. a dataloader (see `segment_anything.utils.transforms`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117521a3",
   "metadata": {
    "id": "117521a3"
   },
   "outputs": [],
   "source": [
    "transformed_boxes = predictor.transform.apply_boxes_torch(input_boxes, image.shape[:2])\n",
    "masks, _, _ = predictor.predict_torch(\n",
    "    point_coords=None,\n",
    "    point_labels=None,\n",
    "    boxes=transformed_boxes,\n",
    "    multimask_output=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8f5d49",
   "metadata": {
    "id": "6a8f5d49"
   },
   "outputs": [],
   "source": [
    "masks.shape  # (batch_size) x (num_predicted_masks_per_input) x H x W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00c3681",
   "metadata": {
    "id": "c00c3681"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(image)\n",
    "for mask in masks:\n",
    "    show_mask(mask.cpu().numpy(), plt.gca(), random_color=True)\n",
    "for box in input_boxes:\n",
    "    show_box(box.cpu().numpy(), plt.gca())\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d74a588-09c6-4d36-a17a-4b3a3675248c",
   "metadata": {
    "id": "4d74a588-09c6-4d36-a17a-4b3a3675248c"
   },
   "source": [
    "## Testing Segmentation generation\n",
    "1. Choose another image and define 2 objects to segment. How well the segmentation worked?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f379f2f9-c27a-49be-ba17-c5f5fea4078c",
   "metadata": {
    "id": "f379f2f9-c27a-49be-ba17-c5f5fea4078c"
   },
   "outputs": [],
   "source": [
    "# image = cv2.imread('images/groceries.jpg')\n",
    "image = cv2.imread('images/dog.jpg')\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "########## CODE HERE  ##########\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "AQxtNjdxDxQB",
   "metadata": {
    "id": "AQxtNjdxDxQB"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(image)\n",
    "for box in input_boxes:\n",
    "    show_box(box.cpu().numpy(), plt.gca())\n",
    "plt.axis('on')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yTPWiFurEeq2",
   "metadata": {
    "id": "yTPWiFurEeq2"
   },
   "outputs": [],
   "source": [
    "predictor.set_image(image)\n",
    "transformed_boxes = predictor.transform.apply_boxes_torch(input_boxes, image.shape[:2])\n",
    "masks, _, _ = predictor.predict_torch(\n",
    "    point_coords=None,\n",
    "    point_labels=None,\n",
    "    boxes = transformed_boxes,\n",
    "    multimask_output=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "F1F2A9jgHH77",
   "metadata": {
    "id": "F1F2A9jgHH77"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(image)\n",
    "for mask in masks:\n",
    "    show_mask(mask.cpu().numpy(), plt.gca(), random_color=True)\n",
    "for box in input_boxes:\n",
    "    show_box(box.cpu().numpy(), plt.gca())\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532f4c69-076c-44ec-b0b2-386c19698783",
   "metadata": {
    "id": "532f4c69-076c-44ec-b0b2-386c19698783"
   },
   "source": [
    "# Fruit Segmentation\n",
    "\n",
    "Now we are gonnna attempt to developeda segmentation systems assisted by a detection model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xjiqha_jIL0V",
   "metadata": {
    "id": "xjiqha_jIL0V"
   },
   "outputs": [],
   "source": [
    "!pip install ultralytics==8.3.94"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wCzytCDUIC9e",
   "metadata": {
    "id": "wCzytCDUIC9e"
   },
   "outputs": [],
   "source": [
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27166cfc-0826-48eb-b279-b0d699cec4e8",
   "metadata": {
    "id": "27166cfc-0826-48eb-b279-b0d699cec4e8"
   },
   "outputs": [],
   "source": [
    "model = YOLO('yolov11_best.pt').to(predictor.device)\n",
    "path_video = './short_video3.mov'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9110ed81-754f-4ee2-975e-12c16d2c459f",
   "metadata": {
    "id": "9110ed81-754f-4ee2-975e-12c16d2c459f"
   },
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(path_video)\n",
    "\n",
    "# Read a frame from the video\n",
    "success, frame = cap.read()\n",
    "\n",
    "\n",
    "if success:\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    shape = frame.shape\n",
    "    frame = frame[:shape[1],:shape[1]]\n",
    "    results = model([frame])\n",
    "    \n",
    "    boxes = results[0].boxes.xyxy\n",
    "    confidences = results[0].boxes.conf\n",
    "    print(\"conf: \",confidences)\n",
    "\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52216a5c-7b1e-473e-a5d0-9cf7e9a0bc4b",
   "metadata": {
    "id": "52216a5c-7b1e-473e-a5d0-9cf7e9a0bc4b"
   },
   "outputs": [],
   "source": [
    "########### CHOOSE threshold ###########\n",
    "threshold = ... \n",
    "input_boxes = boxes[confidences > threshold]\n",
    "transformed_boxes = predictor.transform.apply_boxes_torch(input_boxes, frame.shape[:2])\n",
    "predictor.set_image(frame)\n",
    "masks, _, _ = predictor.predict_torch(\n",
    "    point_coords=None,\n",
    "    point_labels=None,\n",
    "    boxes=transformed_boxes,\n",
    "    multimask_output=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d12875-b5d8-4260-88cb-07a5253a077c",
   "metadata": {
    "id": "94d12875-b5d8-4260-88cb-07a5253a077c"
   },
   "outputs": [],
   "source": [
    "########### Code Here ###########"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca060ab-b556-412d-b5f3-567632842c7d",
   "metadata": {
    "id": "6ca060ab-b556-412d-b5f3-567632842c7d"
   },
   "source": [
    "\n",
    "# Acknowledgement\n",
    "\n",
    "Thanks to the developers of SAM for releasing this model as OPEN SORUCE. For more information see the paper of SAM:\n",
    "\n",
    "```\n",
    "@article{kirillov2023segany,\n",
    "  title={Segment Anything},\n",
    "  author={Kirillov, Alexander and Mintun, Eric and Ravi, Nikhila and Mao, Hanzi and Rolland, Chloe and Gustafson, Laura and Xiao, Tete and Whitehead, Spencer and Berg, Alexander C. and Lo, Wan-Yen and Doll{\\'a}r, Piotr and Girshick, Ross},\n",
    "  journal={arXiv:2304.02643},\n",
    "  year={2023}\n",
    "}\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
